{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211e8878-cd3b-4581-9d8d-484d121d9752",
   "metadata": {},
   "source": [
    "# Your data + LLM\n",
    "\n",
    "- There are many LLMs that are released with pre-determined weights and architrecutres. \n",
    "- They are state of the art and building your own would require an enormous amount of compute/storage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4342c0a-d9cb-4d7c-94c5-b316ccf602ed",
   "metadata": {},
   "source": [
    "### Let us look at how we can connect our database to a LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554c50c-3a53-4e0e-941e-ae0b1af0e857",
   "metadata": {},
   "source": [
    "Setup your OpenAI key, by making an account and adding billing details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53feb7c8-293f-4747-bab3-cbfddbf89924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR_KEY\" # replace with your own API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77ed97e-4f52-4982-8d97-e4e4e3427695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd6969-d3c3-47ff-b146-0d50d3425e0f",
   "metadata": {},
   "source": [
    "Langchain allows you to customize the prompts you input to LLMs. There are few instructions you can provide using a prompt template. Let us define our custom prompt template with:\n",
    "\n",
    "- What the AI does:     \n",
    "It tells us how many goals a player has scored in the Premier League for a given season\n",
    "- Context/Background Information:    \n",
    "Keep the answer relevant only to what the LLM knows\n",
    "- Question:    \n",
    "User input, Example : Mohammed Salah 17/18, Eden Hazard 14/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529f63e4-a849-4b17-8077-4e86af208737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Tell me how many goals this player has scored in the Premier League for a given season, if user enters stop return stop\n",
    "\n",
    "Please answer the question by using your own knowledge about the topic\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4902675d-2c95-436d-9021-d0b5836ca1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and the season Eden hazard 12/13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTell me how many goals this player has scored in the Premier League for a given season, if user enters stop return stop\n",
      "\n",
      "Please answer the question by using your own knowledge about the topic\n",
      "\n",
      "Question: Eden hazard 12/13\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Eden Hazard scored 9 goals in the Premier League for the 2012/2013 season.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and the season Salah 17/18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTell me how many goals this player has scored in the Premier League for a given season, if user enters stop return stop\n",
      "\n",
      "Please answer the question by using your own knowledge about the topic\n",
      "\n",
      "Question: Salah 17/18\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Salah scored 32 goals in the Premier League for the 17/18 season.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and the season Erling Haaland 22/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTell me how many goals this player has scored in the Premier League for a given season, if user enters stop return stop\n",
      "\n",
      "Please answer the question by using your own knowledge about the topic\n",
      "\n",
      "Question: Erling Haaland 22/23\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I'm sorry, but as an AI language model, I don't have real-time information or access to the internet. Therefore, I cannot provide you with the exact number of goals Erling Haaland has scored in the Premier League for the 2022/2023 season. To find out the current goal tally for Erling Haaland, I recommend checking reliable sports news websites or using sports apps that provide up-to-date statistics.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and the season stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTell me how many goals this player has scored in the Premier League for a given season, if user enters stop return stop\n",
      "\n",
      "Please answer the question by using your own knowledge about the topic\n",
      "\n",
      "Question: stop\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(input_variables=[ \"question\"], template = prompt_template)\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    ")\n",
    "\n",
    "while True:\n",
    "    res = chatgpt_chain.predict(question = input('Enter player name and the season'))\n",
    "    if(res == 'Stop'):\n",
    "        break\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372512c-0672-4356-b524-e9386fad0752",
   "metadata": {},
   "source": [
    " OpenAI models are only trained till mid-2021. This means the model does not have access to information post that. It will not provide you with any result or it can also provide you with inaccurate results. How can we solve this problem?\n",
    "## Retrievel Augmentaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa66bf5-613c-4407-ae1c-66bc4ac3dbf2",
   "metadata": {},
   "source": [
    "Let us connect an external database with stats from the 22/23 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cd976a9-0039-4ba0-870f-0b527ecaa62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Goals</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erling Haaland</td>\n",
       "      <td>36</td>\n",
       "      <td>Erling Haaland scored 36 goals in 22/23 season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Kane</td>\n",
       "      <td>30</td>\n",
       "      <td>Harry Kane scored 30 goals in 22/23 season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ivan Toney</td>\n",
       "      <td>20</td>\n",
       "      <td>Ivan Toney scored 20 goals in 22/23 season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mohamed Salah</td>\n",
       "      <td>19</td>\n",
       "      <td>Mohamed Salah scored 19 goals in 22/23 season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Callum Wilson</td>\n",
       "      <td>18</td>\n",
       "      <td>Callum Wilson scored 18 goals in 22/23 season</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Player Goals                                            text\n",
       "0  Erling Haaland    36  Erling Haaland scored 36 goals in 22/23 season\n",
       "1      Harry Kane    30      Harry Kane scored 30 goals in 22/23 season\n",
       "2      Ivan Toney    20      Ivan Toney scored 20 goals in 22/23 season\n",
       "3   Mohamed Salah    19   Mohamed Salah scored 19 goals in 22/23 season\n",
       "4   Callum Wilson    18   Callum Wilson scored 18 goals in 22/23 season"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('stats.csv')\n",
    "df['Goals'] =df['Goals'].astype('str')\n",
    "df['text'] = df['Player'] + ' scored ' + df['Goals'] + ' goals in 22/23 season'\n",
    "df_text = df['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0638c976-cdaf-44b9-977b-3b2ef15da7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Erling Haaland scored 36 goals in 22/23 season',\n",
       " 'Harry Kane scored 30 goals in 22/23 season',\n",
       " 'Ivan Toney scored 20 goals in 22/23 season']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = list(df_text)\n",
    "chunks[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8fee3-6a9d-40d8-ae54-9da1c497abb8",
   "metadata": {},
   "source": [
    "Let us convert our textual information to an embedding, so it can be utilized by our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aadf5b0f-b343-42fe-b401-c6989b823f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "model_name = 'text-embedding-ada-002'\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    ")\n",
    "res = embed.embed_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08932c89-cfb9-469b-ac44-7104475e3a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,\n",
       " [-0.01476236973454253,\n",
       "  0.011140585511691175,\n",
       "  0.02854892197756703,\n",
       "  0.013411237608289267])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[0]), res[0][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff4209-a39d-40c9-ae88-77c6c9000848",
   "metadata": {},
   "source": [
    "### Vector Database - Pinecone\n",
    "\n",
    "- Vector databases help in effecient storage and management of embeddings.     \n",
    "- They also have capabilities to search/cluster embeddings based on metrics.     \n",
    "- Let us store all of our embeddings in a pinecone database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7afdf2d1-23d9-42f5-bbb1-47ac2739e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "index_name = 'premstats'\n",
    "\n",
    "pinecone.init(\n",
    "    api_key='YOUR_KEY',\n",
    "    environment='YOUR_API'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b699467a-714b-4ebe-bfd2-d275d0876820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhoAmIResponse(username='a3a1ead', user_label='epl', projectname='b99002e')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75ec9233-afe1-4bcb-bd72-8b5c3dcccda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb1eb41b-6cfa-458a-88b0-44dd46bc46e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 20}},\n",
       " 'total_vector_count': 20}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.GRPCIndex('premstats')\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90658d-c775-464e-adc1-4c6c8c6a6e2d",
   "metadata": {},
   "source": [
    "Since I have already created my vector database, I have not executed the lines below. If you want to input data into your database, please execute the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cebfb8-477b-4736-ad4a-cdbd608cff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_name = list(df['Player'])\n",
    "text = list(df['text'])\n",
    "chunks = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68557b-e216-4a6f-a6a8-7029990e7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_limit = 10\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "\n",
    "record_texts = text\n",
    "record_metadatas = [{\n",
    "    \"chunk\": j, \"text\": text\n",
    "} for j, text in enumerate(record_texts)]\n",
    "\n",
    "texts.extend(record_texts)\n",
    "metadatas.extend(record_metadatas)\n",
    "\n",
    "if len(texts) > 0:\n",
    "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "    embeds = embed.embed_documents(texts)\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55573ee6-1a26-4385-a361-610b59adeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50f5611a-27b0-44b6-a6b0-4caac12d662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9af23-c2d2-4242-9462-0c0586a9785c",
   "metadata": {},
   "source": [
    "Now let us try to utilize the same prompt template and see what the results look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cb60af2-7425-4a1d-8d1f-261c3810a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Tell me how many goals this player scored in the Premier League for given year\n",
    "\n",
    "If the store does not have the answer, please answer the question by using your own knowledge about the english premier league. \n",
    "If player does not have stats, maybe you could check when they joined and left the club. \n",
    "\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(), \n",
    "                                 chain_type_kwargs=chain_type_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9d6d76b-26f8-4767-a9f8-4a02e43cdce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and season Eden hazard 12/13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eden Hazard scored 9 goals in the 12/13 Premier League season.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and season Salah 17/18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mohamed Salah scored 32 goals in the Premier League for the 17/18 season.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and season Erling Haaland 22/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erling Haaland scored 36 goals in the Premier League for the 22/23 season.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and season stop\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    human_input = input(\"Enter player name and season\")\n",
    "    if(human_input == 'stop'):\n",
    "        break\n",
    "    print(qa.run(human_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf85a0-1353-43b8-b812-ff04072ba9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Our LLM works seamlessly now because it is also retrieveing data from the external vector database that we connected it to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4596cc-64ab-4639-a7d1-c77b2644ee16",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "- LLMS are powerful\n",
    "- Based on what it is trained on, it might have hallucinations.\n",
    "- It can provide inaccurate or absoluelty no result.\n",
    "- Connect LLM to extrernal or domain specific dataset to levarage it fully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
