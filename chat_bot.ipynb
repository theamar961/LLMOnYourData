{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211e8878-cd3b-4581-9d8d-484d121d9752",
   "metadata": {},
   "source": [
    "# Your data + LLM\n",
    "\n",
    "- There are many LLMs that are released with pre-determined weights and architrecutres. \n",
    "- They are state of the art and building your own would require an enormous amount of compute/storage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4342c0a-d9cb-4d7c-94c5-b316ccf602ed",
   "metadata": {},
   "source": [
    "### Let us look at how we can connect our database to a LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554c50c-3a53-4e0e-941e-ae0b1af0e857",
   "metadata": {},
   "source": [
    "Setup your OpenAI key, by making an account and adding billing details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53feb7c8-293f-4747-bab3-cbfddbf89924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR_KEY\" # replace with your own API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f77ed97e-4f52-4982-8d97-e4e4e3427695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd6969-d3c3-47ff-b146-0d50d3425e0f",
   "metadata": {},
   "source": [
    "Let us define a prompt template with:\n",
    "\n",
    "- What the AI does:     \n",
    "It tells us how many goals a player has scored in the Premier League for a given season\n",
    "- Context/Background Information:    \n",
    "Keep the answer relevant only to what the LLM knows\n",
    "- Question:    \n",
    "User input, Example : Mohammed Salah 17/18, Eden Hazard 14/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "529f63e4-a849-4b17-8077-4e86af208737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Tell me how many goals this player has scored in the Premier League for a given season\n",
    "\n",
    "Please answer the question by using your own knowledge about the topic\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4902675d-2c95-436d-9021-d0b5836ca1f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and the season Salah 17/18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTell me how many goals this player has scored in the Premier League for a given season\n",
      "\n",
      "Please answer the question by using your own knowledge about the topic\n",
      "\n",
      "Question: Salah 17/18\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "In the 2017/2018 Premier League season, Mohamed Salah scored a total of 32 goals.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and the season Erling Haaland 22/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTell me how many goals this player has scored in the Premier League for a given season\n",
      "\n",
      "Please answer the question by using your own knowledge about the topic\n",
      "\n",
      "Question: Erling Haaland 22/23\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "As an AI language model, I don't have real-time information or access to current statistics. Therefore, I cannot provide you with the exact number of goals Erling Haaland has scored in the Premier League for the 2022/2023 season. To find the accurate and up-to-date information, I recommend checking reliable sports websites, news outlets, or official Premier League sources.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and the season stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTell me how many goals this player has scored in the Premier League for a given season\n",
      "\n",
      "Please answer the question by using your own knowledge about the topic\n",
      "\n",
      "Question: stop\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I'm sorry, but I cannot provide you with the specific number of goals a player has scored in the Premier League for a given season without any specific information about the player or the season.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m chatgpt_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m      4\u001b[0m     llm\u001b[38;5;241m=\u001b[39mChatOpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m      5\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m      6\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     memory\u001b[38;5;241m=\u001b[39mConversationBufferWindowMemory(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     res \u001b[38;5;241m=\u001b[39m chatgpt_chain\u001b[38;5;241m.\u001b[39mpredict(question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEnter player name and the season\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(res \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(input_variables=[ \"question\"], template = prompt_template)\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    ")\n",
    "\n",
    "while True:\n",
    "    res = chatgpt_chain.predict(question = input('Enter player name and the season'))\n",
    "    if(res == 'stop'):\n",
    "        break\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372512c-0672-4356-b524-e9386fad0752",
   "metadata": {},
   "source": [
    " OpenAI models are only trained till mid-2021. This means the model does not have access to information post that. It will not provide you with any result or it can also provide you with inaccurate results. How can we solve this problem?\n",
    "## Retrievel Augmentaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa66bf5-613c-4407-ae1c-66bc4ac3dbf2",
   "metadata": {},
   "source": [
    "Let us connect an external database with stats from the 22/23 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2cd976a9-0039-4ba0-870f-0b527ecaa62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Goals</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erling Haaland</td>\n",
       "      <td>36</td>\n",
       "      <td>Erling Haaland scored 36 goals in 22/23 season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Kane</td>\n",
       "      <td>30</td>\n",
       "      <td>Harry Kane scored 30 goals in 22/23 season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ivan Toney</td>\n",
       "      <td>20</td>\n",
       "      <td>Ivan Toney scored 20 goals in 22/23 season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mohamed Salah</td>\n",
       "      <td>19</td>\n",
       "      <td>Mohamed Salah scored 19 goals in 22/23 season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Callum Wilson</td>\n",
       "      <td>18</td>\n",
       "      <td>Callum Wilson scored 18 goals in 22/23 season</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Player Goals                                            text\n",
       "0  Erling Haaland    36  Erling Haaland scored 36 goals in 22/23 season\n",
       "1      Harry Kane    30      Harry Kane scored 30 goals in 22/23 season\n",
       "2      Ivan Toney    20      Ivan Toney scored 20 goals in 22/23 season\n",
       "3   Mohamed Salah    19   Mohamed Salah scored 19 goals in 22/23 season\n",
       "4   Callum Wilson    18   Callum Wilson scored 18 goals in 22/23 season"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('stats.csv')\n",
    "df['Goals'] =df['Goals'].astype('str')\n",
    "df['text'] = df['Player'] + ' scored ' + df['Goals'] + ' goals in 22/23 season'\n",
    "df_text = df['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0638c976-cdaf-44b9-977b-3b2ef15da7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Erling Haaland scored 36 goals in 22/23 season',\n",
       " 'Harry Kane scored 30 goals in 22/23 season',\n",
       " 'Ivan Toney scored 20 goals in 22/23 season']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = list(df_text)\n",
    "chunks[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8fee3-6a9d-40d8-ae54-9da1c497abb8",
   "metadata": {},
   "source": [
    "Let us convert our textual information to an embedding, so it can be utilized by our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aadf5b0f-b343-42fe-b401-c6989b823f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "model_name = 'text-embedding-ada-002'\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    ")\n",
    "res = embed.embed_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08932c89-cfb9-469b-ac44-7104475e3a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,\n",
       " [-0.014846178470043785,\n",
       "  0.011162775393497753,\n",
       "  0.028516670241503065,\n",
       "  0.013382822996481001])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[0]), res[0][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff4209-a39d-40c9-ae88-77c6c9000848",
   "metadata": {},
   "source": [
    "### Vector Database - Pinecone\n",
    "\n",
    "- Vector databases help in effecient storage and management of embeddings.     \n",
    "- They also have capabilities to search/cluster embeddings based on metrics.     \n",
    "- Let us store all of our embeddings in a pinecone database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7afdf2d1-23d9-42f5-bbb1-47ac2739e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "index_name = 'premstats'\n",
    "\n",
    "pinecone.init(\n",
    "    api_key='62fa0cb0-aeb2-41ca-903c-6b4e7bdf361f',\n",
    "    environment='us-west4-gcp-free'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b699467a-714b-4ebe-bfd2-d275d0876820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhoAmIResponse(username='a3a1ead', user_label='epl', projectname='b99002e')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75ec9233-afe1-4bcb-bd72-8b5c3dcccda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb1eb41b-6cfa-458a-88b0-44dd46bc46e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 20}},\n",
       " 'total_vector_count': 20}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.GRPCIndex('premstats')\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00cebfb8-477b-4736-ad4a-cdbd608cff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_name = list(df['Player'])\n",
    "text = list(df['text'])\n",
    "chunks = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec68557b-e216-4a6f-a6a8-7029990e7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_limit = 10\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "\n",
    "record_texts = text\n",
    "record_metadatas = [{\n",
    "    \"chunk\": j, \"text\": text\n",
    "} for j, text in enumerate(record_texts)]\n",
    "\n",
    "texts.extend(record_texts)\n",
    "metadatas.extend(record_metadatas)\n",
    "\n",
    "if len(texts) > 0:\n",
    "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "    embeds = embed.embed_documents(texts)\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55573ee6-1a26-4385-a361-610b59adeea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 20}},\n",
       " 'total_vector_count': 20}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50f5611a-27b0-44b6-a6b0-4caac12d662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2cb60af2-7425-4a1d-8d1f-261c3810a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Tell me how many goals this player scored in the Premier League for given year\n",
    "\n",
    "If the store does not have the answer, please answer the question by using your own knowledge about the english premier league. \n",
    "If player does not have stats, maybe you could check when they joined and left the club. \n",
    "\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(), \n",
    "                                 chain_type_kwargs=chain_type_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9d6d76b-26f8-4767-a9f8-4a02e43cdce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and season Salah 17/18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mohamed Salah scored 32 goals in the Premier League for the 17/18 season.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and season Erling Haaland 22/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erling Haaland scored 36 goals in the Premier League for the 22/23 season.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter player name and season stop\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    human_input = input(\"Enter player name and season\")\n",
    "    if(human_input == 'stop'):\n",
    "        break\n",
    "    print(qa.run(human_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4596cc-64ab-4639-a7d1-c77b2644ee16",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "- LLMS are powerful\n",
    "- Based on what it is trained on, it might have hallucinations.\n",
    "- It can provide inaccurate or absoluelty no result.\n",
    "- Connect LLM to extrernal or domain specific dataset to levarage it fully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
